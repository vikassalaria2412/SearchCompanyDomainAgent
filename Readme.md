# Company Search Subsidiaries Domain Agent

This repository contains a web scraping project that leverages large language models (LLMs) to gather and process information from various web sources. The project includes multiple agents for searching, crawling, and extracting data, and it is built using Python.

## Project Structure

- `app.py`: The main entry point for running the application.
- `config.py`: Contains configuration settings such as API keys, endpoints, and other parameters.
- `endpoints.py`: Manages API endpoints and defines routes for the web service.
- `prompts.py`: Stores the prompts used for interacting with the LLMs.
- `requirements.txt`: Lists the Python dependencies required for the project.

### Directories

- `agents/`
  - `extractor_agent.py`: Handles data extraction tasks.
  - `search_agent.py`: Manages search operations across different sources.
  - `web_crawler_agent.py`: Conducts web crawling activities to gather data.
- `artifacts/`
  - Contains logs, JSON files, and other artifacts generated by the web scraping processes.
  - Examples: `webcrawles_logs.txt`, `chain_final_response_latest.json`, `test.html`.
- `models/`
  - `pydantic_models.py`: Defines data models using Pydantic for validation and structure.
- `templates/`
  - `index.html`: Basic HTML template used by the application.

### Documentation & Other Files

- `.env`: Environment variables configuration file.
- `Assignment_Company_Research.pdf`: Provides research documentation.
- `ResponseDocumentation.docx`: Contains detailed documentation on the project's responses.
- `response_screenshot_web_1.png`: Screenshot demonstrating the output of the agent.

## Setup & Installation

1. Clone the repository:
   ```bash
   git clone https://github.com/vikassalaria2412/SearchCompanyDomainAgent.git

   ```

2. Create a virtual environment:
   ```bash
   conda create -n agentenv python==3.11
   conda activate   agentenv
   ```

3. Install the dependencies:
   ```bash
   pip install -r requirements.txt
   ```

4. Set up environment variables:
   - Create the `.env` file and fill in the necessary environment variables:
        ```
        TAVILY_API_KEY=********************* # change this to your Tavily API key
        PERPLEXITY_API_KEY=***************** # change this to your Perplexity API key
        OPENAI_API_KEY=********************* # change this to your OpenAI API key
        WEB_CRAWL_URL_PROVIDER="openai/gpt-4o" 
        OPENAI_MODEL="gpt-4o"
        ```
   

5. Run the application:
   ```bash
   python app.py
   ```

## Usage

Once the application is running, you can access the web service through the defined API endpoints. The project is designed to perform automated web scraping and data extraction using various agents.

## Contributing

If you want to contribute to this project, feel free to open an issue or submit a pull request. Contributions are welcome!

## License

This project is licensed under the MIT License. See the [LICENSE](LICENSE) file for more details.